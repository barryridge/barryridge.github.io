
@inproceedings{ridge_action-grounded_2013,
	title = {Action-grounded push affordance bootstrapping of unknown objects},
	doi = {10.1109/IROS.2013.6696751},
	abstract = {When it comes to learning how to manipulate objects from experience with minimal prior knowledge, robots encounter significant challenges. When the objects are unknown to the robot, the lack of prior object models demands a robust feature descriptor such that the robot can reliably compare objects and the effects of their manipulation. In this paper, using an experimental platform that gathers 3-D data from the Kinect RGB-D sensor, as well as push action trajectories from a tracking system, we address these issues using an action-grounded 3-D feature descriptor. Rather than using pose-invariant visual features, as is often the case with object recognition, we ground the features of objects with respect to their manipulation, that is, by using shape features that describe the surface of an object relative to the push contact point and direction. Using this setup, object push affordance learning trials are performed by a human and both pre-push and post-push object features are gathered, as well as push action trajectories. A self-supervised multi-view online learning algorithm is employed to bootstrap both the discovery of affordance classes in the post-push view, as well as a discriminative model for predicting them in the pre-push view. Experimental results demonstrate the effectiveness of self-supervised class discovery, class prediction and feature relevance determination on a collection of unknown objects.},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Ridge, Barry and Ude, Aleš},
	month = nov,
	year = {2013},
	keywords = {3D data, action-grounded 3D feature descriptor, action-grounded push affordance bootstrapping, class prediction, discriminative model, feature extraction, feature relevance determination, kinect RGB-D sensor, learning (artificial intelligence), mobile robots, object push affordance learning trials, object recognition, object tracking, pose-invariant visual features, post-push view, pre-push view, prior object models, Prototypes, push action trajectory, push contact direction, push contact point, robots, robust feature descriptor, selfsupervised class discovery, selfsupervised multiview online learning algorithm, Shape, shape features, tracking system, Training, Trajectory, unknown objects, Vectors},
	pages = {2791--2798},
	file = {0913-2.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/9XJ4XCHG/0913-2.pdf:application/pdf;IEEE Xplore Abstract Record:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/IJQZVPXF/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/WZSUHNWJ/Ridge and Ude - 2013 - Action-grounded push affordance bootstrapping of u.pdf:application/pdf}
}

@inproceedings{ridge_comparison_2015,
	title = {Comparison of {Action}-{Grounded} and {Non}-{Action}-{Grounded} 3-{D} {Shape} {Features} for {Object} {Affordance} {Classification}},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7251523},
	abstract = {Recent work in robotics, particularly in the domains of object manipulation and affordance learning, has seen the development of action-grounded features, that is, object features
that are defined dynamically with respect to manipulation actions.
Rather than using pose-invariant features, as is often the case with
object recognition, such features are grounded with respect to the
manipulation of the object, for instance, by using shape features
that describe the surface of an object relative to the push contact
point and direction. In this paper we provide an experimental
comparison between action-grounded features and non-grounded
features in an object affordance classification setting. Using an
experimental platform that gathers 3-D data from the Kinect
RGB-D sensor, as well as push action trajectories from an elec-
tromagnetic tracking system, we provide experimental results that demonstrate the effectiveness of this action-grounded approach
across a range of state-of-the-art classifiers.},
	urldate = {2015-12-02},
	booktitle = {The 17th {International} {Conference} on {Advanced} {Robotics} ({ICAR}), {Istanbul}, {Turkey}},
	author = {Ridge, Barry and Ugur, Emre and Ude, Aleš},
	year = {2015},
	file = {[PDF] from ijs.si:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/K66RFFQB/Ridge et al. - 2015 - Comparison of Action-Grounded and Non-Action-Groun.pdf:application/pdf;Snapshot:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/7CH32VNK/login.html:text/html}
}

@inproceedings{skocaj_framework_2007,
	address = {St. Lambrecht, Austria},
	title = {A framework for continuous learning of simple visual concepts},
	url = {http://vicos.fri.uni-lj.si/data/publications/skocajCVWW07.pdf},
	abstract = {We present a continuous learning framework for
learning simple visual concepts and its implementation in
an artificial cognitive system. The main goal is to learn as-
sociations between automatically extracted visual features
and words that describe the scene in an open-ended, contin-
uous manner. In particular, we address the problem of cross-
modal learning of elementary visual properties and spatial
relations; we show that the same learning mechanism can
be used to learn both types of concepts. We introduce and
analyse several learning modes requiring different levels of
tutor supervision, ranging from a completely tutor driven to
a completely autonomous exploratory approach.},
	booktitle = {Proceedings of the {Twelveth} {Computer} {Vision} {Winter} {Workshop} ({CVWW})},
	author = {Skočaj, Danijel and Ridge, Barry and Berginc, Gregor and Leonardis, Aleš},
	year = {2007},
	pages = {99--105},
	file = {skocajCVWW07.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/RHZ84AMH/skocajCVWW07.pdf:application/pdf}
}

@inproceedings{ridge_relevance_2012,
	address = {Mala Nedelja, Slovenia},
	title = {Relevance {Determination} for {Learning} {Vector} {Quantization} using the {Fisher} {Criterion} {Score}},
	abstract = {Two new feature relevance determination algorithms are proposed for learning vector quantization. The algorithms exploit the positioning of the prototype vectors in the input feature space to estimate Fisher criterion scores for the input dimensions during training. These scores are used to form online estimates of weighting factors for an adaptive metric that accounts for dimensional relevance with respect to classifier output. The methods offer theoretical advantages over previously proposed LVQ relevance determination techniques based on gradient descent, as well as performance advantages as demonstrated in experiments on various datasets including a visual dataset from a cognitive robotics object affordance learning experiment.},
	booktitle = {Proceedings of the {Seventeenth} {Computer} {Vision} {Winter} {Workshop} ({CVWW})},
	author = {Ridge, Barry and Leonardis, Aleš and Skočaj, Danijel},
	month = feb,
	year = {2012}
}

@inproceedings{ridge_unsupervised_2009,
	address = {Eibiswald, Austria},
	title = {Unsupervised {Learning} of {Basic} {Object} {Affordances} from {Object} {Properties}},
	url = {http://vicos.fri.uni-lj.si/data/publications/ridgeCVWW09.pdf},
	abstract = {Affordance learning has, in recent years, been generating heightened interest in both the cognitive vision and developmental robotics communities. In this paper we describe the development of a system that uses a robotic arm to interact with household objects on a table surface while observing the interactions using camera systems. Various computer vision methods are used to derive, firstly, object property features from intensity images and range data gathered before interaction and, subsequently, result features derived from video sequences gathered during and after interaction. We propose a novel affordance learning algorithm that automatically discretizes the result feature space in an unsupervised manner to form affordance classes that are then used as labels to train a supervised classifier in the object property feature space. This classifier may then be used to predict affordance classes, grounded in the result space, of novel objects based on object property observations.},
	booktitle = {Proceedings of the {Fourteenth} {Computer} {Vision} {Winter} {Workshop} ({CVWW})},
	author = {Ridge, Barry and Skočaj, Danijel and Leonardis, Aleš},
	year = {2009},
	pages = {21--27},
	file = {ridgeCVWW09.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/AEARGRW3/ridgeCVWW09.pdf:application/pdf}
}

@inproceedings{ridge_self-supervised_2010,
	address = {Anchorage, USA},
	title = {Self-supervised cross-modal online learning of basic object affordances for developmental robotic systems},
	isbn = {978-1-4244-5038-1},
	doi = {10.1109/ROBOT.2010.5509544},
	abstract = {For a developmental robotic system to function successfully in the real world, it is important that it be able to form its own internal representations of affordance classes based on observable regularities in sensory data. Usually successful classifiers are built using labeled training data, but it is not always realistic to assume that labels are available in a developmental robotics setting. There does, however, exist an advantage in this setting that can help circumvent the absence of labels: co-occurrence of correlated data across separate sensory modalities over time. The main contribution of this paper is an online classifier training algorithm based on Kohonen's learning vector quantization (LVQ) that, by taking advantage of this co-occurrence information, does not require labels during training, either dynamically generated or otherwise. We evaluate the algorithm in experiments involving a robotic arm that interacts with various household objects on a table surface where camera systems extract features for two separate visual modalities. It is shown to improve its ability to classify the affordances of novel objects over time, coming close to the performance of equivalent fully-supervised algorithms.},
	language = {English},
	booktitle = {Proceedings of the 2010 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Ridge, Barry and Skočaj, Danijel and Leonardis, Aleš},
	month = may,
	year = {2010},
	keywords = {affordance classes, Cameras, camera systems, cooccurrence information, Data mining, developmental robotic systems, feature extraction, Information science, Kohonen learning vector quantization, learning (artificial intelligence), manipulators, online classifier training algorithm, robotic arm, Robotics and automation, Robot sensing systems, Robot vision systems, self-organising feature maps, self-supervised cross-modal online learning, Shape, USA Councils, Vector quantization, visual modalities},
	pages = {5047--5054},
	file = {IEEE Xplore Full Text PDF:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/BFVI76CJ/Ridge - 2010 - Self-supervised cross-modal online learning of bas.pdf:application/pdf}
}

@inproceedings{nemec_transfer_2013,
	title = {Transfer of assembly operations to new workpiece poses by adaptation to the desired force profile},
	doi = {10.1109/ICAR.2013.6766568},
	abstract = {In this paper we propose a new algorithm that can be used for adaptation of robot trajectories in automated assembly tasks. Initial trajectories and forces are obtained by demonstration and iteratively adapted to specific environment configurations. The algorithm adapts Cartesian space trajectories to match the forces recorded during the human demonstration. Experimentally we show the effectiveness of our approach on learning of Peg-in-Hole (PiH) task. We performed our experiments on two different robotic platforms with workpieces of different shapes.},
	booktitle = {2013 16th {International} {Conference} on {Advanced} {Robotics} ({ICAR})},
	author = {Nemec, Bojan and Abu-Dakka, Fares J. and Ridge, Barry and Ude, Aleš and Jorgensen, Jimmy A. and Savarimuthu, Thiusius Rajeeth and Jouffroy, Jerome and Petersen, Henrik G. and Kruger, Norbert},
	month = nov,
	year = {2013},
	keywords = {Assembly Operations, Cartesian space trajectories, Force, Force measurement, peg-in-hole task, Quaternions, robot automated assembly tasks, robotic assembly, robotic platforms, robots, robot trajectories, Torque, Torque measurement, Trajectory, trajectory control, workpiece},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/MK3KHNHC/login.html:text/html}
}

@inproceedings{ridge_towards_2008,
	address = {Brighton, UK},
	title = {Towards {Learning} {Basic} {Object} {Affordances} from {Object} {Properties}},
	url = {http://vicos.fri.uni-lj.si/data/publications/ridgeEpiRob08.pdf},
	abstract = {The capacity for learning to recognize and exploit environmental affordances is an important consideration for the design of current and future developmental robotic systems. We present a system that uses a robotic arm, camera systems and self-organizing maps to learn basic affordances of objects.},
	booktitle = {Proceedings of the {Eight} {International} {Conference} on {Epigenetic} {Robotics} ({EpiRob})},
	author = {Ridge, Barry and Skočaj, Danijel and Leonardis, Aleš},
	year = {2008},
	pages = {155--156},
	file = {ridgeEpiRob08.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/GVUJAZPS/ridgeEpiRob08.pdf:application/pdf}
}

@inproceedings{ridge_system_2008,
	address = {Karlsruhe, Germany},
	title = {A system for learning basic object affordances using a self-organizing map},
	url = {http://vicos.fri.uni-lj.si/data/publications/ridgeCogSys08.pdf},
	abstract = {When a cognitive system encounters particular objects, it needs to know what effect each of its possible actions will have on the state of each of those objects in order to be able to make effective decisions and achieve its goals. Moreover, it should be able to generalize effectively so that when it encounters novel objects, it is able to estimate what effect its actions will have on them based on its experiences with previously encountered similar objects. This idea is encapsulated by the term “affordance”, e.g. “a ball affords being rolled to the right when pushed from the left.” In this paper, we discuss the development of a cognitive vision platform that uses a robotic arm to interact with household objects in an attempt to learn some of their basic affordance properties. We outline the various sensor and effector module competencies that were needed to achieve this and describe an experiment that uses a self-organizing map to integrate these modalities in a working affordance learning system.},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Cognitive} {Systems} ({CogSys})},
	author = {Ridge, Barry and Skočaj, Danijel and Leonardis, Aleš},
	year = {2008},
	pages = {65--70},
	file = {ridgeCogSys08.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/64B3U4FB/ridgeCogSys08.pdf:application/pdf}
}

@inproceedings{skocaj_interaktiven_2007,
	address = {Portorož, Slovenia},
	title = {Interaktiven sistem za kontinuirano učenje vizualnih konceptov},
	url = {http://vicos.fri.uni-lj.si/data/publications/skocaj_erk07.pdf},
	abstract = {We present an artifficial cognitive system for learning visual concepts. It comprises of vision, communication and manipulation subsystems, which provide visual input, enable verbal and non-verbal communication with a tutor and allow interaction with a given scene. The main goal is to learn associations between automatically extracted visual features and words that describe the scene in an open-ended, continuous manner. In particular, we address the problem of cross-modal learning of visual properties and spatial relations and analyse several learning modes requiring different levels of tutor supervision.},
	booktitle = {Proceedings of the {Sixteenth} {Electrotechnical} and {Computer} {Science} {Conference} ({ERK})},
	author = {Skočaj, Danijel and Vrečko, Alen and Kristan, Matej and Ridge, Barry and Berginc, Gregor and Leonardis, Aleš},
	year = {2007},
	pages = {167--170},
	file = {skocaj_erk07.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/RTD2R6TB/skocaj_erk07.pdf:application/pdf}
}

@phdthesis{ridge_techniques_2006,
	type = {M.{Phil}. {Thesis}},
	title = {Techniques for {Computing} {Exact} {Hausdorff} {Measure} with {Application} to a {Sierpinski} {Sponge} in {R}{\textasciicircum}3},
	abstract = {In this dissertation we aim to perform a detailed study of techniques for the analysis of the exact s-dimensional Hausdorff measure of fractal sets and try to provide a reason-
ably comprehensive review of the required background. An emphasis is placed on results pertaining to local density of sets and we show how these provide a link to the more global concept of Hausdorff measure. A new result is provided which states that if K
is a self-similar set satisfying the open set condition, then H{\textasciicircum}s(K ∩ U) ≤ {\textbar}U{\textbar}{\textasciicircum}s for all Borel U , also implying that D{\textasciicircum}s\_c(K, x) ≤ 1 for all x, where H{\textasciicircum}s(E) and D{\textasciicircum}s\_c(E, x) refer to
the s-dimensional Hausdorff measure of some set E and the local convex density of E at a point x respectively. Based on the work of Zuoling Zhou and Min Wu, we provide new calculations for the exact Hausdorff measure of both a Sierpinski carpet in R{\textasciicircum}2 and a Sierpinski sponge in R{\textasciicircum}3 . In the final chapter we take a look at how the Hausdorff measure behaves when measuring the invariant sets associated with special types of iterated
function systems known as iterated function systems with condensation and also provide a brief discussion on the calculation of the packing measure of a self-similar set.},
	school = {School of Mathematics and Statistics, University of St Andrews},
	author = {Ridge, Barry},
	year = {2006},
	file = {thesis.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/IUTKH8ZT/thesis.pdf:application/pdf}
}

@inproceedings{skocaj_system_2007,
	address = {Bielefeld, Germany},
	title = {A {System} for {Continuous} {Learning} of {Visual} {Concepts}},
	url = {http://vicos.fri.uni-lj.si/data/publications/skocajICVS07.pdf},
	abstract = {We present an artifficial cognitive system for learning visual concepts. It comprises of vision, communication and manipulation sub- systems, which provide visual input, enable verbal and non-verbal com munication with a tutor and allow interaction with a given scene. The main goal is to learn associations between automatically extracted visual features and words that describe the scene in an open-ended, continuous manner. In particular, we address the problem of cross-modal learning of visual properties and spatial relations. We introduce and analyse several learning modes requiring different levels of tutor supervision.},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Computer} {Vision} {Systems} ({ICVS})},
	author = {Skočaj, Danijel and Berginc, Gregor and Ridge, Barry and Štimec, Aleš and Jogan, Matjaz and Vanek, Ondrej and Leonardis, Aleš and Hutter, Manuela and Hawes, Nick},
	month = mar,
	year = {2007},
	pages = {1--10},
	file = {skocajICVS07.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/9SWEAB8B/skocajICVS07.pdf:application/pdf}
}

@inproceedings{skocaj_different_2006,
	address = {Portorož, Slovenia.},
	title = {On different modes of continuous learning of visual properties},
	url = {http://vicos.fri.uni-lj.si/data/publications/skocajERK06.pdf},
	abstract = {Za vsak spoznavni sistem, tudi umetni, je zelo
pomembno, da se je sposoben učiti in pridobljeno
znanje nadgrajevati. V tem članku obravnavamo ra-
zlične načine inkrementalnega učenja, ki to omogoča.
Predstavimo učenje, pri katerem uporabnik oz.
učitelj zagotovi umetnemu sistemu vse potrebne in-
formacije, ki jih potrebuje, nato učenje, pri katerem
sistem zahteva od uporabnika informacije glede na
stopnjo nedoločenosti, ter učenje, pri katerem sis-
tem nadgrajuje svoje znanje popolnoma brez pomoči
uporabnika. V članku tudi predstavimo metodo,
ki omogoča inkrementalno učenje vizualnih lastnosti
predmetov na vse tri načine. Z eksperimentalnimi
rezultati vse tri pristope tudi ovrednotimo.},
	booktitle = {Proceedings of the {Fifteenth} {Electrotechnical} and {Computer} {Science} {Conference} ({ERK})},
	author = {Skočaj, Danijel and Ridge, Barry and Leonardis, Aleš},
	year = {2006},
	pages = {105--108},
	file = {skocajERK06.pdf:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/ZE2GETQU/skocajERK06.pdf:application/pdf}
}

@article{ridge_self-supervised_2015,
	title = {Self-{Supervised} {Online} {Learning} of {Basic} {Object} {Push} {Affordances}},
	volume = {12},
	issn = {, 1729-8814},
	url = {http://arx.sagepub.com/content/12/3/24},
	doi = {10.5772/59654},
	abstract = {Continuous learning of object affordances in a cognitive robot is a challenging problem, the solution to which arguably requires a developmental approach. In this paper, we describe scenarios where robotic systems interact with household objects by pushing them using robot arms while observing the scene with cameras, and which must incrementally learn, without external supervision, both the effect classes that emerge from these interactions as well as a discriminative model for predicting them from object properties. We formalize the scenario as a multi-view learning problem where data co-occur over two separate data views over time, and we present an online learning framework that uses a self-supervised form of learning vector quantization to build the discriminative model. In various experiments, we demonstrate the effectiveness of this approach in comparison with related supervised methods using data from experiments performed using two different robotic platforms.},
	language = {en},
	number = {3},
	urldate = {2016-10-12},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Ridge, Barry and Leonardis, Aleš and Ude, Aleš and Deniša, Miha and Skočaj, Danijel},
	month = mar,
	year = {2015},
	keywords = {affordances, Cognitive and Developmental Robotics, online learning, self-supervised learning},
	pages = {24},
	file = {Full Text PDF:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/RH898CN3/Ridge et al. - 2015 - Self-Supervised Online Learning of Basic Object Pu.pdf:application/pdf;Snapshot:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/3JCS9K34/24.html:text/html}
}

@phdthesis{ridge_learning_2014,
	type = {Ph.{D}. {Thesis}},
	title = {Learning {Basic} {Object} {Affordances} in a {Robotic} {System}},
	url = {http://eprints.fri.uni-lj.si/2888/},
	abstract = {One of the fundamental enabling mechanisms of human and animal intelligence, and equally, one of the great challenges of modern day autonomous robotics is the ability to perceive and exploit environmental affordances. To recognise how you can interact with objects in the world, that is to recognise what they afford you, is to speak the language of cause and effect, and as with most languages, practice is one of the most important paths to understanding. This is clear from early childhood development. Through countless hours of motor babbling, children gain a wealth of experience from basic interactions with the world around them, and from there they are able to learn basic affordances and gradually more complex ones. Implementing such affordance learning capabilities in a robot, however, is no trivial matter. This is an inherently multi-disciplinary challenge, drawing on such fields as autonomous robotics, computer vision, machine learning, artificial intelligence, psychology, neuroscience, and others. In this thesis, we attempt to study the problem of affordance learning by embracing its multi-disciplinary nature. We use a real robotic system to perform experiments using household objects. Camera systems record images and video of these interactions from which computer vision algorithms extract interesting features. These features are used as data for a machine learning algorithm that was inspired in part by ideas from psychology and neuroscience. The learning algorithm is perhaps the main focal point of the work presented here. It is a self-supervised multi-view online learner that dynamically forms categories in one data view, or sensory modality, that are used to drive supervised learning in another. While useful in and of itself, the self-supervised learner can potentially benefit from certain augmentations, particularly over shorter training periods. To this end, we also propose two novel feature relevance determination methods that can be applied to the self-supervised learner. With regard to robotic experiments, we make use of two different robotic setups, each of which involves a robot arm operating in an experimental environment with a flat table surface, with camera systems pointing at the scene. Objects placed in the environment can be manipulated, generally pushed, by the arm, and the camera systems can record image and video data of the interaction. One of the camera systems in one of the setups is a stereo camera, and another in the other setup is an RGB-D sensor, thus allowing for the extraction of range data and 3-D point cloud data. In the thesis, we describe computer vision algorithms for extracting both salient object features from the static images and point cloud data, and effect features from the video data of the object in motion. A series of experiments are described that evaluate the proposed feature relevance algorithms, the self-supervised multi-view learning algorithm, and the application of these to real-world object push affordance learning problems using the robotic setups. Some surprising results emerge from these experiments and as well as those, under the conditions we present, our framework is shown to be able to autonomously discover object affordance categories in data, predict the affordance categories of novel objects and determine the most relevant object properties for discriminating between those categories.},
	urldate = {2016-10-31},
	school = {University of Ljubljana},
	author = {Ridge, Barry},
	month = dec,
	year = {2014},
	file = {Full Text PDF:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/UFB4JXHN/Ridge - 2014 - Learning Basic Object Affordances in a Robotic Sys.pdf:application/pdf;Snapshot:/home/barry/.mozilla/firefox/ipqhgwvn.default/zotero/storage/DVXAKMVS/2888.html:text/html}
}