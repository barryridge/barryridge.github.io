<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Barry Ridge | publications</title>
  <meta name="description" content="Barry Ridge, postdoctoral researcher. Learning to let robots learn.
">

  <link rel="stylesheet" href="https://barryridge.github.io/https://barryridge.github.io//assets/css/main.css">
  <link rel="canonical" href="https://barryridge.github.io/https://barryridge.github.io//publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://barryridge.github.io/https://barryridge.github.io//">about</a>

        <!-- Blog -->
        <a class="page-link" href="https://barryridge.github.io/https://barryridge.github.io//blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="https://barryridge.github.io/https://barryridge.github.io//projects/">projects</a>
          
        
          
            <a class="page-link" href="https://barryridge.github.io/https://barryridge.github.io//publications/">publications</a>
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Journal publications, conference publications and theses.</h5>
  </header>

  <article class="post-content publications clearfix">
    
<h3 class="year">2015</h3>
<ol class="bibliography"><li>

<div id="ridge_comparison_2015">
  
    
      <a href="https://barryridge.github.io/papers/ridge_comparison_2015.pdf" target="_blank"><span class="title">Comparison of Action-Grounded and Non-Action-Grounded 3-D Shape Features for Object Affordance Classification.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="http://www.cmpe.boun.edu.tr/~emre/" target="_blank">Ugur, Emre</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.ijs.si/%7Eaude/" target="_blank">Ude, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In The 17th International Conference on Advanced Robotics (ICAR), Istanbul, Turkey</em>
    
    
      2015
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recent work in robotics, particularly in the domains of object manipulation and affordance learning, has seen the development of action-grounded features, that is, object features
that are defined dynamically with respect to manipulation actions.
Rather than using pose-invariant features, as is often the case with
object recognition, such features are grounded with respect to the
manipulation of the object, for instance, by using shape features
that describe the surface of an object relative to the push contact
point and direction. In this paper we provide an experimental
comparison between action-grounded features and non-grounded
features in an object affordance classification setting. Using an
experimental platform that gathers 3-D data from the Kinect
RGB-D sensor, as well as push action trajectories from an elec-
tromagnetic tracking system, we provide experimental results that demonstrate the effectiveness of this action-grounded approach
across a range of state-of-the-art classifiers.</p>
  </span>
  
</div>
</li>
<li>

<div id="ridge_self-supervised_2015">
  
    
      <a href="https://barryridge.github.io/papers/ridge_self-supervised_2015.pdf" target="_blank"><span class="title">Self-Supervised Online Learning of Basic Object Push Affordances.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>,
            
          
        
      
        
          
            
              <a href="https://www.ijs.si/%7Eaude/" target="_blank">Ude, Aleš</a>,
            
          
        
      
        
          
            
              <a href="http://abr.ijs.si/miha-denisa/en" target="_blank">Deniša, Miha</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>International Journal of Advanced Robotic Systems</em>
    
    
      2015
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Continuous learning of object affordances in a cognitive robot is a challenging problem, the solution to which arguably requires a developmental approach. In this paper, we describe scenarios where robotic systems interact with household objects by pushing them using robot arms while observing the scene with cameras, and which must incrementally learn, without external supervision, both the effect classes that emerge from these interactions as well as a discriminative model for predicting them from object properties. We formalize the scenario as a multi-view learning problem where data co-occur over two separate data views over time, and we present an online learning framework that uses a self-supervised form of learning vector quantization to build the discriminative model. In various experiments, we demonstrate the effectiveness of this approach in comparison with related supervised methods using data from experiments performed using two different robotic platforms.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2014</h3>
<ol class="bibliography"><li>

<div id="ridge_learning_2014">
  
    
      <a href="https://barryridge.github.io/papers/ridge_learning_2014.pdf" target="_blank"><span class="title">Learning Basic Object Affordances in a Robotic System.</span></a>
    
    <span class="author">
      
        
          
          
            <em>Ridge, Barry</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
      2014
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>One of the fundamental enabling mechanisms of human and animal intelligence, and equally, one of the great challenges of modern day autonomous robotics is the ability to perceive and exploit environmental affordances. To recognise how you can interact with objects in the world, that is to recognise what they afford you, is to speak the language of cause and effect, and as with most languages, practice is one of the most important paths to understanding. This is clear from early childhood development. Through countless hours of motor babbling, children gain a wealth of experience from basic interactions with the world around them, and from there they are able to learn basic affordances and gradually more complex ones. Implementing such affordance learning capabilities in a robot, however, is no trivial matter. This is an inherently multi-disciplinary challenge, drawing on such fields as autonomous robotics, computer vision, machine learning, artificial intelligence, psychology, neuroscience, and others. In this thesis, we attempt to study the problem of affordance learning by embracing its multi-disciplinary nature. We use a real robotic system to perform experiments using household objects. Camera systems record images and video of these interactions from which computer vision algorithms extract interesting features. These features are used as data for a machine learning algorithm that was inspired in part by ideas from psychology and neuroscience. The learning algorithm is perhaps the main focal point of the work presented here. It is a self-supervised multi-view online learner that dynamically forms categories in one data view, or sensory modality, that are used to drive supervised learning in another. While useful in and of itself, the self-supervised learner can potentially benefit from certain augmentations, particularly over shorter training periods. To this end, we also propose two novel feature relevance determination methods that can be applied to the self-supervised learner. With regard to robotic experiments, we make use of two different robotic setups, each of which involves a robot arm operating in an experimental environment with a flat table surface, with camera systems pointing at the scene. Objects placed in the environment can be manipulated, generally pushed, by the arm, and the camera systems can record image and video data of the interaction. One of the camera systems in one of the setups is a stereo camera, and another in the other setup is an RGB-D sensor, thus allowing for the extraction of range data and 3-D point cloud data. In the thesis, we describe computer vision algorithms for extracting both salient object features from the static images and point cloud data, and effect features from the video data of the object in motion. A series of experiments are described that evaluate the proposed feature relevance algorithms, the self-supervised multi-view learning algorithm, and the application of these to real-world object push affordance learning problems using the robotic setups. Some surprising results emerge from these experiments and as well as those, under the conditions we present, our framework is shown to be able to autonomously discover object affordance categories in data, predict the affordance categories of novel objects and determine the most relevant object properties for discriminating between those categories.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2013</h3>
<ol class="bibliography"><li>

<div id="ridge_action-grounded_2013">
  
    
      <a href="https://barryridge.github.io/papers/ridge_action-grounded_2013.pdf" target="_blank"><span class="title">Action-grounded push affordance bootstrapping of unknown objects.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
          
            
              <a href="https://www.ijs.si/%7Eaude/" target="_blank">Ude, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
    
    
      2013
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>When it comes to learning how to manipulate objects from experience with minimal prior knowledge, robots encounter significant challenges. When the objects are unknown to the robot, the lack of prior object models demands a robust feature descriptor such that the robot can reliably compare objects and the effects of their manipulation. In this paper, using an experimental platform that gathers 3-D data from the Kinect RGB-D sensor, as well as push action trajectories from a tracking system, we address these issues using an action-grounded 3-D feature descriptor. Rather than using pose-invariant visual features, as is often the case with object recognition, we ground the features of objects with respect to their manipulation, that is, by using shape features that describe the surface of an object relative to the push contact point and direction. Using this setup, object push affordance learning trials are performed by a human and both pre-push and post-push object features are gathered, as well as push action trajectories. A self-supervised multi-view online learning algorithm is employed to bootstrap both the discovery of affordance classes in the post-push view, as well as a discriminative model for predicting them in the pre-push view. Experimental results demonstrate the effectiveness of self-supervised class discovery, class prediction and feature relevance determination on a collection of unknown objects.</p>
  </span>
  
</div>
</li>
<li>

<div id="nemec_transfer_2013">
  
    
      <a href="https://barryridge.github.io/papers/nemec_transfer_2013.pdf" target="_blank"><span class="title">Transfer of assembly operations to new workpiece poses by adaptation to the desired force profile.</span></a>
    
    <span class="author">
      
        
          
            
              <a href="http://abr.ijs.si/bojan-nemec/en" target="_blank">Nemec, Bojan</a>,
            
          
        
      
        
          
            
              <a href="http://uc3m.academia.edu/FaresAbudakka/Papers" target="_blank">Abu-Dakka, Fares J.</a>,
            
          
        
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.ijs.si/%7Eaude/" target="_blank">Ude, Aleš</a>,
            
          
        
      
        
          
            
              <a href="http://caro.sdu.dk/index.php/contacts/8-jimmy-alison-rytz" target="_blank">Jorgensen, Jimmy A.</a>,
            
          
        
      
        
          
            
              <a href="http://www.sdu.dk/staff/trs" target="_blank">Savarimuthu, Thiusius Rajeeth</a>,
            
          
        
      
        
          
            
              <a href="http://www.sdu.dk/staff/jerome" target="_blank">Jouffroy, Jerome</a>,
            
          
        
      
        
          
            
              <a href="http://www.sdu.dk/staff/hgp" target="_blank">Petersen, Henrik G.</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.sdu.dk/staff/norbert" target="_blank">Kruger, Norbert</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In 2013 16th International Conference on Advanced Robotics (ICAR)</em>
    
    
      2013
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper we propose a new algorithm that can be used for adaptation of robot trajectories in automated assembly tasks. Initial trajectories and forces are obtained by demonstration and iteratively adapted to specific environment configurations. The algorithm adapts Cartesian space trajectories to match the forces recorded during the human demonstration. Experimentally we show the effectiveness of our approach on learning of Peg-in-Hole (PiH) task. We performed our experiments on two different robotic platforms with workpieces of different shapes.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2012</h3>
<ol class="bibliography"><li>

<div id="ridge_relevance_2012">
  
    
      <a href="https://barryridge.github.io/papers/ridge_relevance_2012.pdf" target="_blank"><span class="title">Relevance Determination for Learning Vector Quantization using the Fisher Criterion Score.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Seventeenth Computer Vision Winter Workshop (CVWW)</em>
    
    
      2012
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Two new feature relevance determination algorithms are proposed for learning vector quantization. The algorithms exploit the positioning of the prototype vectors in the input feature space to estimate Fisher criterion scores for the input dimensions during training. These scores are used to form online estimates of weighting factors for an adaptive metric that accounts for dimensional relevance with respect to classifier output. The methods offer theoretical advantages over previously proposed LVQ relevance determination techniques based on gradient descent, as well as performance advantages as demonstrated in experiments on various datasets including a visual dataset from a cognitive robotics object affordance learning experiment.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2010</h3>
<ol class="bibliography"><li>

<div id="ridge_self-supervised_2010">
  
    
      <a href="https://barryridge.github.io/papers/ridge_self-supervised_2010.pdf" target="_blank"><span class="title">Self-supervised cross-modal online learning of basic object affordances for developmental robotic systems.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the 2010 IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2010
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>For a developmental robotic system to function successfully in the real world, it is important that it be able to form its own internal representations of affordance classes based on observable regularities in sensory data. Usually successful classifiers are built using labeled training data, but it is not always realistic to assume that labels are available in a developmental robotics setting. There does, however, exist an advantage in this setting that can help circumvent the absence of labels: co-occurrence of correlated data across separate sensory modalities over time. The main contribution of this paper is an online classifier training algorithm based on Kohonen’s learning vector quantization (LVQ) that, by taking advantage of this co-occurrence information, does not require labels during training, either dynamically generated or otherwise. We evaluate the algorithm in experiments involving a robotic arm that interacts with various household objects on a table surface where camera systems extract features for two separate visual modalities. It is shown to improve its ability to classify the affordances of novel objects over time, coming close to the performance of equivalent fully-supervised algorithms.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2009</h3>
<ol class="bibliography"><li>

<div id="ridge_unsupervised_2009">
  
    
      <a href="https://barryridge.github.io/papers/ridge_unsupervised_2009.pdf" target="_blank"><span class="title">Unsupervised Learning of Basic Object Affordances from Object Properties.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fourteenth Computer Vision Winter Workshop (CVWW)</em>
    
    
      2009
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Affordance learning has, in recent years, been generating heightened interest in both the cognitive vision and developmental robotics communities. In this paper we describe the development of a system that uses a robotic arm to interact with household objects on a table surface while observing the interactions using camera systems. Various computer vision methods are used to derive, firstly, object property features from intensity images and range data gathered before interaction and, subsequently, result features derived from video sequences gathered during and after interaction. We propose a novel affordance learning algorithm that automatically discretizes the result feature space in an unsupervised manner to form affordance classes that are then used as labels to train a supervised classifier in the object property feature space. This classifier may then be used to predict affordance classes, grounded in the result space, of novel objects based on object property observations.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2008</h3>
<ol class="bibliography"><li>

<div id="ridge_towards_2008">
  
    
      <a href="https://barryridge.github.io/papers/ridge_towards_2008.pdf" target="_blank"><span class="title">Towards Learning Basic Object Affordances from Object Properties.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Eight International Conference on Epigenetic Robotics (EpiRob)</em>
    
    
      2008
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The capacity for learning to recognize and exploit environmental affordances is an important consideration for the design of current and future developmental robotic systems. We present a system that uses a robotic arm, camera systems and self-organizing maps to learn basic affordances of objects.</p>
  </span>
  
</div>
</li>
<li>

<div id="ridge_system_2008">
  
    
      <a href="https://barryridge.github.io/papers/ridge_system_2008.pdf" target="_blank"><span class="title">A system for learning basic object affordances using a self-organizing map.</span></a>
    
    <span class="author">
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the First International Conference on Cognitive Systems (CogSys)</em>
    
    
      2008
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>When a cognitive system encounters particular objects, it needs to know what effect each of its possible actions will have on the state of each of those objects in order to be able to make effective decisions and achieve its goals. Moreover, it should be able to generalize effectively so that when it encounters novel objects, it is able to estimate what effect its actions will have on them based on its experiences with previously encountered similar objects. This idea is encapsulated by the term “affordance”, e.g. “a ball affords being rolled to the right when pushed from the left.” In this paper, we discuss the development of a cognitive vision platform that uses a robotic arm to interact with household objects in an attempt to learn some of their basic affordance properties. We outline the various sensor and effector module competencies that were needed to achieve this and describe an experiment that uses a self-organizing map to integrate these modalities in a working affordance learning system.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2007</h3>
<ol class="bibliography"><li>

<div id="skocaj_framework_2007">
  
    
      <a href="https://barryridge.github.io/papers/skocaj_framework_2007.pdf" target="_blank"><span class="title">A framework for continuous learning of simple visual concepts.</span></a>
    
    <span class="author">
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.researchgate.net/profile/Gregor_Berginc" target="_blank">Berginc, Gregor</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Twelveth Computer Vision Winter Workshop (CVWW)</em>
    
    
      2007
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a continuous learning framework for
learning simple visual concepts and its implementation in
an artificial cognitive system. The main goal is to learn as-
sociations between automatically extracted visual features
and words that describe the scene in an open-ended, contin-
uous manner. In particular, we address the problem of cross-
modal learning of elementary visual properties and spatial
relations; we show that the same learning mechanism can
be used to learn both types of concepts. We introduce and
analyse several learning modes requiring different levels of
tutor supervision, ranging from a completely tutor driven to
a completely autonomous exploratory approach.</p>
  </span>
  
</div>
</li>
<li>

<div id="skocaj_interaktiven_2007">
  
    
      <a href="https://barryridge.github.io/papers/skocaj_interaktiven_2007.pdf" target="_blank"><span class="title">Interaktiven sistem za kontinuirano učenje vizualnih konceptov.</span></a>
    
    <span class="author">
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
            
              <a href="https://www.researchgate.net/profile/Alen_Vrecko" target="_blank">Vrečko, Alen</a>,
            
          
        
      
        
          
            
              <a href="http://www.vicos.si/People/Matejk" target="_blank">Kristan, Matej</a>,
            
          
        
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.researchgate.net/profile/Gregor_Berginc" target="_blank">Berginc, Gregor</a>,
            
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Sixteenth Electrotechnical and Computer Science Conference (ERK)</em>
    
    
      2007
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present an artifficial cognitive system for learning visual concepts. It comprises of vision, communication and manipulation subsystems, which provide visual input, enable verbal and non-verbal communication with a tutor and allow interaction with a given scene. The main goal is to learn associations between automatically extracted visual features and words that describe the scene in an open-ended, continuous manner. In particular, we address the problem of cross-modal learning of visual properties and spatial relations and analyse several learning modes requiring different levels of tutor supervision.</p>
  </span>
  
</div>
</li>
<li>

<div id="skocaj_system_2007">
  
    
      <a href="https://barryridge.github.io/papers/skocaj_system_2007.pdf" target="_blank"><span class="title">A System for Continuous Learning of Visual Concepts.</span></a>
    
    <span class="author">
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
            
              <a href="https://www.researchgate.net/profile/Gregor_Berginc" target="_blank">Berginc, Gregor</a>,
            
          
        
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
            
              <a href="https://www.fri.uni-lj.si/si/ales-stimec/" target="_blank">Štimec, Aleš</a>,
            
          
        
      
        
          
            
              <a href="http://www.fri.uni-lj.si/en/matjaz-jogan/default.html" target="_blank">Jogan, Matjaz</a>,
            
          
        
      
        
          
            
              <a href="http://cs.fel.cvut.cz/en/people/vanekon2" target="_blank">Vanek, Ondrej</a>,
            
          
        
      
        
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>,
            
          
        
      
        
          
            
              <a href="http://manooh.com/#home" target="_blank">Hutter, Manuela</a>,
            
          
        
      
        
          
          
            
              <a href="http://www.cs.bham.ac.uk/~nah/" target="_blank">Hawes, Nick</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fifth International Conference on Computer Vision Systems (ICVS)</em>
    
    
      2007
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present an artifficial cognitive system for learning visual concepts. It comprises of vision, communication and manipulation sub- systems, which provide visual input, enable verbal and non-verbal com munication with a tutor and allow interaction with a given scene. The main goal is to learn associations between automatically extracted visual features and words that describe the scene in an open-ended, continuous manner. In particular, we address the problem of cross-modal learning of visual properties and spatial relations. We introduce and analyse several learning modes requiring different levels of tutor supervision.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2006</h3>
<ol class="bibliography"><li>

<div id="ridge_techniques_2006">
  
    
      <a href="https://barryridge.github.io/papers/ridge_techniques_2006.pdf" target="_blank"><span class="title">Techniques for Computing Exact Hausdorff Measure with Application to a Sierpinski Sponge in R^3.</span></a>
    
    <span class="author">
      
        
          
          
            <em>Ridge, Barry</em>
          
        
      
    </span>

    
    <span class="periodical">
    
    
      2006
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this dissertation we aim to perform a detailed study of techniques for the analysis of the exact s-dimensional Hausdorff measure of fractal sets and try to provide a reason-
ably comprehensive review of the required background. An emphasis is placed on results pertaining to local density of sets and we show how these provide a link to the more global concept of Hausdorff measure. A new result is provided which states that if K
is a self-similar set satisfying the open set condition, then H^s(K ∩ U) ≤ \textbarU\textbar^s for all Borel U , also implying that D^s_c(K, x) ≤ 1 for all x, where H^s(E) and D^s_c(E, x) refer to
the s-dimensional Hausdorff measure of some set E and the local convex density of E at a point x respectively. Based on the work of Zuoling Zhou and Min Wu, we provide new calculations for the exact Hausdorff measure of both a Sierpinski carpet in R^2 and a Sierpinski sponge in R^3 . In the final chapter we take a look at how the Hausdorff measure behaves when measuring the invariant sets associated with special types of iterated
function systems known as iterated function systems with condensation and also provide a brief discussion on the calculation of the packing measure of a self-similar set.</p>
  </span>
  
</div>
</li>
<li>

<div id="skocaj_different_2006">
  
    
      <a href="https://barryridge.github.io/papers/skocaj_different_2006.pdf" target="_blank"><span class="title">On different modes of continuous learning of visual properties.</span></a>
    
    <span class="author">
      
        
          
            
              <a href="http://www.vicos.si/People/Danijels" target="_blank">Skočaj, Danijel</a>,
            
          
        
      
        
          
            <em>Ridge, Barry</em>,
          
        
      
        
          
          
            
              <a href="https://www.cs.bham.ac.uk/~leonarda/" target="_blank">Leonardis, Aleš</a>
            
          
        
      
    </span>

    
    <span class="periodical">
    
      <em>In Proceedings of the Fifteenth Electrotechnical and Computer Science Conference (ERK)</em>
    
    
      2006
    
    </span>
    

    
  

  <span class="links">
  
    [<a class="abstract">Abs</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Za vsak spoznavni sistem, tudi umetni, je zelo
pomembno, da se je sposoben učiti in pridobljeno
znanje nadgrajevati. V tem članku obravnavamo ra-
zlične načine inkrementalnega učenja, ki to omogoča.
Predstavimo učenje, pri katerem uporabnik oz.
učitelj zagotovi umetnemu sistemu vse potrebne in-
formacije, ki jih potrebuje, nato učenje, pri katerem
sistem zahteva od uporabnika informacije glede na
stopnjo nedoločenosti, ter učenje, pri katerem sis-
tem nadgrajuje svoje znanje popolnoma brez pomoči
uporabnika. V članku tudi predstavimo metodo,
ki omogoča inkrementalno učenje vizualnih lastnosti
predmetov na vse tri načine. Z eksperimentalnimi
rezultati vse tri pristope tudi ovrednotimo.</p>
  </span>
  
</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
  	<p class="small">
    © Copyright 2016 Barry Ridge.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
</p>
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://barryridge.github.io/https://barryridge.github.io//assets/js/common.js"></script>

<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<script src="https://barryridge.github.io/https://barryridge.github.io//assets/js/katex.js"></script>

<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://barryridge.github.io/https://barryridge.github.io//assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://barryridge.github.io/https://barryridge.github.io//assets/css/academicons.min.css">


  </body>

</html>
